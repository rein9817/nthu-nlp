# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U3Pq97lYQ6HUOPdRV2xHmTx2K4Z7LpOC

## Part I: Data Pre-processing
"""

import pandas as pd

# Download the Google Analogy dataset
!wget http://download.tensorflow.org/data/questions-words.txt

# Preprocess the dataset
file_name = "questions-words"
with open(f"{file_name}.txt", "r") as f:
    data = f.read().splitlines()

# check data from the first 10 entries
for entry in data[:10]:
    print(entry)

# TODO1: Write your code here for processing data to pd.DataFrame
# Please note that the first five mentions of ": " indicate `semantic`,
# and the remaining nine belong to the `syntatic` category.
questions = []
categories = []
sub_categories = []

current_sub_category = ""
semantic_count = 0

for entry in data:
    entry = entry.strip()
    if not entry:
        continue

    if entry.startswith(':'):
        current_sub_category = entry
        semantic_count += 1
        continue

    parts = entry.split()
    if len(parts) == 4:
        questions.append(' '.join(parts))
        categories.append('Semantic' if semantic_count <= 5 else 'Syntactic')
        sub_categories.append(current_sub_category)

# Create the dataframe
df = pd.DataFrame(
    {
        "Question": questions,
        "Category": categories,
        "SubCategory": sub_categories,
    }
)

df.head()

df.to_csv(f"{file_name}.csv", index=False)

"""## Part II: Use pre-trained word embeddings
- After finish Part I, you can run Part II code blocks only.
"""

!pip install gensim

import pandas as pd
import numpy as np
import gensim.downloader
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

data = pd.read_csv("questions-words.csv")

MODEL_NAME = "glove-wiki-gigaword-100"
# You can try other models.
# https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models

# Load the pre-trained model (using GloVe vectors here)
model = gensim.downloader.load(MODEL_NAME)
print("The Gensim model loaded successfully!")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []

for analogy in tqdm(data["Question"]):
      # TODO2: Write your code here to use pre-trained word embeddings for getting predictions of the analogy task.
      # You should also preserve the gold answers during iterations for evaluations later.
      """ Hints
      # Unpack the analogy (e.g., "man", "woman", "king", "queen")
      # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
      # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776
      # Mikolov et al., 2013: big - biggest and small - smallest
      # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).
      """
      words = analogy.split()
      if len(words) != 4:
            continue
      word_a, word_b, word_c, word_d = words  # Unpack the analogy
      try:
          predicted_words = model.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)
          predicted_word = predicted_words[0][0]  # Get the top predicted word
      except KeyError:
         predicted_word = None  # Handle cases where words are not in vocabulary
      preds.append(predicted_word)
      golds.append(word_d)

# Perform evaluations. You do not need to modify this block!!

def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
    return np.mean(gold == pred)

golds_np, preds_np = np.array(golds), np.array(preds)
data = pd.read_csv("questions-words.csv")

# Evaluation: categories
for category in data["Category"].unique():
    mask = data["Category"] == category
    golds_cat, preds_cat = golds_np[mask], preds_np[mask]
    acc_cat = calculate_accuracy(golds_cat, preds_cat)
    print(f"Category: {category}, Accuracy: {acc_cat * 100}%")

# Evaluation: sub-categories
for sub_category in data["SubCategory"].unique():
    mask = data["SubCategory"] == sub_category
    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
    print(f"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%")

# Collect words from Google Analogy dataset
SUB_CATEGORY = ": family"

# TODO3: Plot t-SNE for the words in the SUB_CATEGORY `: family`
family_words = set()
family_data = data[data["SubCategory"] == SUB_CATEGORY]
for analogy in family_data["Question"]:
    words = analogy.split()
    if len(words) == 4:
        family_words.update(words)

# Filter words that exist in the model
family_words_in_model = [word for word in family_words if word in model]

print(f"Family words found in model: {len(family_words_in_model)}")
print(f"Words: {family_words_in_model}")

if len(family_words_in_model) > 1:
    word_vectors = np.array([model[word] for word in family_words_in_model])
    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(family_words_in_model)-1))
    word_vectors_2d = tsne.fit_transform(word_vectors)

    plt.figure(figsize=(12, 8))
    plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], alpha=0.7, s=100)

    for i, word in enumerate(family_words_in_model):
        plt.annotate(word, (word_vectors_2d[i, 0], word_vectors_2d[i, 1]),
                    xytext=(5, 5), textcoords='offset points', fontsize=10, alpha=0.8)

plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.title("Word Relationships from Google Analogy Task")
plt.show()
plt.savefig("word_relationships.png", bbox_inches="tight")

"""### Part III: Train your own word embeddings

### Get the latest English Wikipedia articles and do sampling.
- Usually, we start from Wikipedia dump (https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2). However, the downloading step will take very long. Also, the cleaning step for the Wikipedia corpus ([`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus)) will take much time. Therefore, we provide cleaned files for you.
"""

# Download the split Wikipedia files
# Each file contain 562365 lines (articles).
!gdown --id 1jiu9E1NalT2Y8EIuWNa1xf2Tw1f1XuGd -O wiki_texts_part_0.txt.gz
!gdown --id 1ABblLRd9HXdXvaNv8H9fFq984bhnowoG -O wiki_texts_part_1.txt.gz
!gdown --id 1z2VFNhpPvCejTP5zyejzKj5YjI_Bn42M -O wiki_texts_part_2.txt.gz
!gdown --id 1VKjded9BxADRhIoCzXy_W8uzVOTWIf0g -O wiki_texts_part_3.txt.gz
!gdown --id 16mBeG26m9LzHXdPe8UrijUIc6sHxhknz -O wiki_texts_part_4.txt.gz

# Download the split Wikipedia files
# Each file contain 562365 lines (articles), except the last file.
!gdown --id 17JFvxOH-kc-VmvGkhG7p3iSZSpsWdgJI -O wiki_texts_part_5.txt.gz
!gdown --id 19IvB2vOJRGlrYulnTXlZECR8zT5v550P -O wiki_texts_part_6.txt.gz
!gdown --id 1sjwO8A2SDOKruv6-8NEq7pEIuQ50ygVV -O wiki_texts_part_7.txt.gz
!gdown --id 1s7xKWJmyk98Jbq6Fi1scrHy7fr_ellUX -O wiki_texts_part_8.txt.gz
!gdown --id 17eQXcrvY1cfpKelLbP2BhQKrljnFNykr -O wiki_texts_part_9.txt.gz
!gdown --id 1J5TAN6bNBiSgTIYiPwzmABvGhAF58h62 -O wiki_texts_part_10.txt.gz

# Extract the downloaded wiki_texts_parts files.
!gunzip -k wiki_texts_part_*.gz

# Combine the extracted wiki_texts_parts files.
!cat wiki_texts_part_*.txt > wiki_texts_combined.txt

# Check the first ten lines of the combined file
!head -n 10 wiki_texts_combined.txt

"""Please note that we used the default parameters of [`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus) for cleaning the Wiki raw file. Thus, words with one character were discarded."""

import random

random.seed(33)
wiki_txt_path = "wiki_texts_combined.txt"
output_path = "wiki_texts_sampled.txt"
ratio = 0.05

total_lines = 5623655

sample_size = int(total_lines * ratio)

sample_indices = set(random.sample(range(total_lines), sample_size))

sampled_lines = []
with open(wiki_txt_path, "r", encoding="utf-8") as f:
    for i, line in enumerate(f):
        if i in sample_indices:
            sampled_lines.append(line)


with open(output_path, "w", encoding="utf-8") as output_file:
    output_file.writelines(sampled_lines)

from gensim.models import Word2Vec

class SentenceIterator:
    def __init__(self, filename, max_lines=None):
        self.filename = filename
        self.max_lines = max_lines

    def __iter__(self):
        count = 0
        with open(self.filename, "r", encoding="utf-8") as f:
            for line in f:
                words = line.lower().split()
                if len(words) > 1:
                    yield words
                    count += 1
                    if self.max_lines and count >= self.max_lines:
                        break

sentences = SentenceIterator("wiki_texts_sampled.txt", max_lines=100000)


my_model = Word2Vec(
    sentences=sentences,
    vector_size=100,
    window=5,
    min_count=5,
    workers=2,
    epochs=5,
    seed=42
)

my_model.save("my_word2vec_model.bin")

data = pd.read_csv("questions-words.csv")

preds = []
golds = []

for analogy in tqdm(data["Question"]):
        try:
            words = analogy.split()
            if len(words) != 4:
                continue

            word_a, word_b, word_c, word_d = words
            golds.append(word_d)
            word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()

            if word_a in my_model.wv and word_b in my_model.wv and word_c in my_model.wv:
                try:
                    result = my_model.wv.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)
                    predicted_word = result[0][0]  # Get the top prediction
                    preds.append(predicted_word)
                except KeyError:
                    preds.append("UNK")
            else:
                preds.append("UNK")

        except Exception as e:
            preds.append("UNK")
            golds.append("UNK")

# Collect words from Google Analogy dataset
SUB_CATEGORY = ": family"

# TODO7: Plot t-SNE for the words in the SUB_CATEGORY `: family`
family_data = data[data["SubCategory"] == SUB_CATEGORY]
family_words = set()
for analogy in family_data["Question"]:
    words = analogy.split()
    if len(words) == 4:
        # Convert to lowercase to match trained model
        family_words.update([word.lower() for word in words])

family_words_in_model = [word for word in family_words if word in my_model.wv]

print(f"Family words found in your trained model: {len(family_words_in_model)}")
print(f"Words: {family_words_in_model}")

if len(family_words_in_model) > 1:
    word_vectors = np.array([my_model.wv[word] for word in family_words_in_model])

    perplexity_value = min(30, len(family_words_in_model)-1)
    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
    word_vectors_2d = tsne.fit_transform(word_vectors)

    plt.figure(figsize=(12, 8))
    plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], alpha=0.7, s=100, c='blue')

    for i, word in enumerate(family_words_in_model):
        plt.annotate(word, (word_vectors_2d[i, 0], word_vectors_2d[i, 1]),
                    xytext=(5, 5), textcoords='offset points', fontsize=12, alpha=0.8)

plt.title("Word Relationships from Google Analogy Task")
plt.show()
plt.savefig("word_relationships.png", bbox_inches="tight")