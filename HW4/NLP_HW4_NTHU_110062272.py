# -*- coding: utf-8 -*-
"""main (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xmF-h76a52i5ToC1v_AIWhnsMP6XyqXS

# RAG using Langchain

## Packages loading & import
"""

!pip install "langchain-core>=0.2.0,<0.3.0" \
             "langchain>=0.2.0,<0.3.0" \
             "langchain-community>=0.2.0,<0.3.0" \
             "langchain-huggingface>=0.0.3,<0.1.0" \
             "langchain-chroma>=0.1.0,<0.2.0" \
             "langchain-ollama>=0.1.0,<0.2.0" \
             "langchain-text-splitters>=0.2.0,<0.3.0" \
             "transformers>=4.39.0" \
             "accelerate>=0.28.0" \
             "sentence-transformers" \
             rank-bm25 \
             huggingface_hub \
             tqdm \
             beautifulsoup4

import os
import json
import bs4
import nltk
import torch
import pickle
import numpy as np

# from pyserini.index import IndexWriter
# from pyserini.search import SimpleSearcher
from numpy.linalg import norm
from rank_bm25 import BM25Okapi
from nltk.tokenize import word_tokenize

from langchain_community.llms import Ollama
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain.vectorstores import Chroma
from sentence_transformers import SentenceTransformer
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.embeddings import JinaEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter
from langchain.docstore.document import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.document_loaders import WebBaseLoader
from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer

from tqdm import tqdm

nltk.download('punkt')
nltk.download('punkt_tab')

"""## Hugging face login
- Please apply the model first: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct
- If you haven't been granted access to this model, you can use other LLM model that doesn't have to apply.
- You must save the hf token otherwise you need to regenrate the token everytime.
- When using Ollama, no login is required to access and utilize the llama model.
"""

from huggingface_hub import login

hf_token = "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"
login(token=hf_token, add_to_git_credential=True)

!huggingface-cli whoami

"""## TODO1: Set up the environment of Ollama

### Introduction to Ollama
- Ollama is a platform designed for running and managing large language models (LLMs) directly **on local devices**, providing a balance between performance, privacy, and control.
- There are also other tools support users to manage LLM on local devices and accelerate it like *vllm*, *Llamafile*, *GPT4ALL*...etc.

### Launch colabxterm
"""

# Commented out IPython magic to ensure Python compatibility.
# TODO1-1: You should install colab-xterm and launch it.
# Write your commands here.
!pip install colab-xterm
# %load_ext colabxterm

# TODO1-2: You should install Ollama.
# You may need root privileges if you use a local machine instead of Colab.
!curl -fsSL https://ollama.com/install.sh | sh

# Commented out IPython magic to ensure Python compatibility.
# %xterm

# TODO1-3: Pull Llama3.2:1b via Ollama and start the Ollama service in the xterm
# Write your commands in the xterm
# ollama pull llama3.2:1b
# nohup ollama serve > ~/ollama.log 2>&1 &

"""## Ollama testing
You can test your Ollama status with the following cells.
"""

# Setting up the model that this tutorial will use
MODEL = "llama3.2:1b" # https://ollama.com/library/llama3.2:3b
EMBED_MODEL = "jinaai/jina-embeddings-v2-base-en"

# Initialize an instance of the Ollama model
llm = Ollama(model=MODEL)
# Invoke the model to generate responses
response = llm.invoke("What is the capital of Taiwan?")
print(response)

"""## Build a simple RAG system by using LangChain

### TODO2: Load the cat-facts dataset and prepare the retrieval database
"""

!wget https://huggingface.co/ngxson/demo_simple_rag_py/resolve/main/cat-facts.txt

# TODO2-1: Load the cat-facts dataset (as `refs`, which is a list of strings for all the cat facts)
# Write your code here
with open('cat-facts.txt', 'r') as f:
    refs = [line.strip() for line in f.readlines()]

from langchain_core.documents import Document
docs = [Document(page_content=doc, metadata={"id": i}) for i, doc in enumerate(refs)]

# Create an embedding model
model_kwargs = {'trust_remote_code': True}
encode_kwargs = {'normalize_embeddings': False}
embeddings_model = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

# TODO2-2: Prepare the retrieval database
# You should create a Chroma vector store.
# search_type can be “similarity” (default), “mmr”, or “similarity_score_threshold”
EXPERIMENT_FORMAT = "original"
if EXPERIMENT_FORMAT == "lowercase":
    processed_docs = [Document(
        page_content=doc.lower(),
        metadata={"id": i}
    ) for i, doc in enumerate(refs)]

elif EXPERIMENT_FORMAT == "no_punct":
    processed_docs = [Document(
        page_content=doc.translate(str.maketrans('', '', string.punctuation)),
        metadata={"id": i}
    ) for i, doc in enumerate(refs)]

elif EXPERIMENT_FORMAT == "normalized":
    processed_docs = [Document(
        page_content=' '.join(
            doc.lower()
            .translate(str.maketrans('', '', string.punctuation))
            .split()
        ),
        metadata={"id": i}
    ) for i, doc in enumerate(refs)]

else:  # "original"
    processed_docs = [Document(
        page_content=doc,
        metadata={"id": i}
    ) for i, doc in enumerate(refs)]


vector_store = Chroma.from_documents(
    # Write your code here
    documents=docs,
    collection_name="cat-facts",
    embedding=embeddings_model
)
# retriever = vector_store.as_retriever(
#     search_type="similarity",
#     search_kwargs={"k": 5}
# )
# retriever = vector_store.as_retriever(
#     search_type="similarity_score_threshold",
#     search_kwargs={
#         "score_threshold": 0.5,
#         "k": 7
#     }
# )

retriever = vector_store.as_retriever(search_type="mmr", k=5)

"""### Prompt setting"""

# TODO3: Set up the `system_prompt` and configure the prompt.
system_prompt = (
    "You are an assistant for question-answering tasks about cats. "
    "Use the following pieces of retrieved context to answer the question. "
    "The answer should be concise and extracted directly from the context. "
    "If the answer contains specific terms, numbers, or technical words, "
    "provide them exactly as they appear in the context. "
    "If you cannot find the answer in the context, say 'I cannot find the answer'.\n\n"
    "Context: {context}"
)

# Few-shot
# system_prompt= (
#     "You are an assistant for question-answering tasks about cats. "
#     "Use the following pieces of retrieved context to answer the question. "
#     "The answer should be concise and extracted directly from the context.\n\n"

#     "Example 1:\n"
#     "Context: A group of cats is called a \"clowder.\"\n"
#     "Question: What is a group of cats called?\n"
#     "Answer: Clowder\n\n"

#     "Example 2:\n"
#     "Context: Cats make about 100 different sounds. Dogs make only about 10.\n"
#     "Question: How many sounds do cats make?\n"
#     "Answer: About 100\n\n"

#     "Example 3:\n"
#     "Context: The technical term for a cat's hairball is a \"bezoar.\"\n"
#     "Question: What is the technical term for a cat's hairball?\n"
#     "Answer: Bezoar\n\n"

#     "Now answer the following question using the same format:\n\n"
#     "Context: {context}"
# )

# CoT
# system_prompt = (
#     "You are an assistant for question-answering tasks about cats. "
#     "Use the following pieces of retrieved context to answer the question.\n\n"

#     "Instructions:\n"
#     "1. First, carefully read the context to find relevant information\n"
#     "2. Identify the key terms, numbers, or facts that answer the question\n"
#     "3. Extract the answer directly from the context\n"
#     "4. Provide a concise answer using exact words from the context\n"
#     "5. If you cannot find the answer, say 'I cannot find the answer'\n\n"

#     "Think step by step:\n"
#     "- What is the question asking for?\n"
#     "- Which part of the context is most relevant?\n"
#     "- What is the exact answer from that part?\n\n"

#     "Context: {context}\n\n"
#     "Now provide your answer:"
# )


prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "Based on the context, please answer the following question:\n{input}"),
    ]
)

"""- For the vectorspace, the common algorithm would be used like Faiss, Chroma...(https://python.langchain.com/docs/integrations/vectorstores/) to deal with the extreme huge database."""

# TODO4: Build and run the RAG system
# TODO4-1: Load the QA chain
# You should create a chain for passing a list of Documents to a model.
question_answer_chain = create_stuff_documents_chain(
    llm=llm,
    prompt=prompt
)

# TODO4-2: Create retrieval chain
# You should create retrieval chain that retrieves documents and then passes them on.
chain = create_retrieval_chain(
    retriever=retriever,
    combine_docs_chain=question_answer_chain
)

# Question (queries) and answer pairs
# Write your code here
# Please load the questions_answers.txt file and prepare the `queries` and `answers` lists.

with open('questions_answers.txt', 'r', encoding='utf-8') as f:
    content = f.read().strip()
    lines = content.split('\n')

lines = [line.strip() for line in lines if line.strip()]

print(f"Total lines: {len(lines)}")

queries = []
answers = []

for i in range(0, len(lines) - 1, 2):
    queries.append(lines[i])
    answers.append(lines[i + 1])

if len(lines) % 2 == 1:
    print(f"Warning: Last question has no answer, skipping")

print(f"Loaded {len(queries)} questions")
if len(queries) > 0:
    print(f"Example Q: {queries[0]}")
    print(f"Example A: {answers[0]}")

import os
import json
import re
from tqdm import tqdm
import random


DOCUMENT_ORDER = "default"  #"default", "reversed", "random", "middle_first"

ADD_COUNTERFACTUAL = False
COUNTERFACT_POSITION = "beginning"  # "beginning", "middle", "end"

OUTPUT_FILE = "NLP_HW4_NTHU_110062272.json"

def generate_counterfactual(correct_doc, query):
    content = correct_doc
    numbers = re.findall(r'\d+', content)
    if numbers:
        for num in numbers:
            try:
                original_num = int(num)
                wrong_num = str(original_num + random.choice([10, -10, 5, -5]))
                content = content.replace(num, wrong_num, 1)
                break
            except:
                pass

    replacements = {
        "bezoar": "furball",
        "clowder": "pack",
        "two thirds": "one third",
        "100": "50",
        "31 mph": "20 mph",
        "500 million": "200 million",
        "sweet tooth": "sense of sweetness",
        "mutation": "evolution",
        "head level": "head up and down",
    }

    for original, wrong in replacements.items():
        if original.lower() in content.lower():
            if original[0].isupper():
                wrong = wrong.capitalize()
            content = content.replace(original, wrong)
            break

    if " can " in content.lower():
        content = content.replace(" can ", " cannot ", 1)
    elif " cannot " in content.lower():
        content = content.replace(" cannot ", " can ", 1)

    return content

def normalize_text(text: str) -> str:
    if text is None:
        return ""
    text = text.strip().lower()
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"[^a-z0-9\s]", "", text)
    return text

def is_match(prediction: str, ground_truth: str) -> bool:
    pred_norm = normalize_text(prediction)
    gt_norm = normalize_text(ground_truth)

    if not pred_norm or not gt_norm:
        return False

    if gt_norm in pred_norm or pred_norm in gt_norm:
        return True

    pred_nums = re.findall(r"\d+", pred_norm)
    gt_nums = re.findall(r"\d+", gt_norm)
    if pred_nums and gt_nums:
        for pn in pred_nums:
            for gn in gt_nums:
                if pn == gn:
                    return True

    return False

results = []

print("\nStarting RAG evaluation loop...\n")

correct_count = 0
recall_at_1_count = 0
recall_at_5_count = 0

for i, query in tqdm(enumerate(queries), total=len(queries)):
    ground_truth = answers[i]

    context_docs = retriever.invoke(query)
    if ADD_COUNTERFACTUAL and len(context_docs) > 0:
        from langchain_core.documents import Document

        original_content = context_docs[0].page_content
        counterfactual_content = generate_counterfactual(original_content, query)

        counterfact_doc = Document(
            page_content=counterfactual_content,
            metadata={"id": "counterfactual", "is_counterfact": True}
        )

        if COUNTERFACT_POSITION == "beginning":
            context_docs = [counterfact_doc] + context_docs
        elif COUNTERFACT_POSITION == "middle":
            mid = len(context_docs) // 2
            context_docs = context_docs[:mid] + [counterfact_doc] + context_docs[mid:]
        else:  # "end"
            context_docs = context_docs + [counterfact_doc]

    if DOCUMENT_ORDER == "reversed":
        context_docs_reordered = list(reversed(context_docs))
    elif DOCUMENT_ORDER == "random":
        context_docs_reordered = context_docs.copy()
        random.shuffle(context_docs_reordered)
    else:  # "default"
        context_docs_reordered = context_docs

    response_answer = question_answer_chain.invoke({
            "input": query,
            "context": context_docs_reordered
    })
    prediction = response_answer.strip() if isinstance(response_answer, str) else response_answer.get("answer", "")

    is_correct = False
    if isinstance(ground_truth, list):
        for answer in ground_truth:
            if is_match(prediction, answer):
                is_correct = True
                break
    else:
        is_correct = is_match(prediction, ground_truth)

    if is_correct:
        correct_count += 1

    if ADD_COUNTERFACTUAL and not is_correct:
        if len(context_docs_reordered) > 0:
            first_doc_is_cf = context_docs_reordered[0].metadata.get("is_counterfact", False)

    # ------ Recall@1 ------
    rec_1 = 0
    if len(context_docs_reordered) > 0:
        top_doc_content = context_docs_reordered[0].page_content
        top_norm = normalize_text(top_doc_content)

        if isinstance(ground_truth, list):
            for answer in ground_truth:
                if normalize_text(answer) in top_norm:
                    rec_1 = 1
                    recall_at_1_count += 1
                    break
        else:
            if normalize_text(ground_truth) in top_norm:
                rec_1 = 1
                recall_at_1_count += 1

    # ------ Recall@5 ------
    rec_5 = 0
    for doc in context_docs_reordered[:5]:
        doc_norm = normalize_text(doc.page_content)
        found = False

        if isinstance(ground_truth, list):
            for answer in ground_truth:
                if normalize_text(answer) in doc_norm:
                    found = True
                    break
        else:
            if normalize_text(ground_truth) in doc_norm:
                found = True

        if found:
            rec_5 = 1
            recall_at_5_count += 1
            break

    retrieved_ids = [doc.metadata.get("id") for doc in context_docs_reordered]

    result_item = {
        "Index": i,
        "Query": query,
        "Ground_Truth": ground_truth,
        "Prediction": prediction,
        "Is_Correct": is_correct,
        "Retrieved_IDs": retrieved_ids,
        "Recall_1": rec_1,
        "Recall_5": rec_5,
        "Retrieved_Contexts": [
            {
                "doc_id": doc.metadata.get("id", "unknown"),
                "content": doc.page_content,
            } for doc in context_docs_reordered
        ]
    }
    results.append(result_item)

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=4, ensure_ascii=False)

total = len(results)
if total > 0:
    accuracy = correct_count / total
    avg_r1 = recall_at_1_count / total
    avg_r5 = recall_at_5_count / total

    print("\n" + "=" * 80)
    print("EVALUATION SUMMARY")
    print("=" * 80)

    print(f"\n結果:")
    print(f"  Total Questions:        {total}")
    print(f"  Correct Answers:        {correct_count}")
    print(f"\nMetrics:")
    print(f"  Accuracy:   {accuracy:.2%} ({correct_count}/{total})")
    print(f"  Recall@1:   {avg_r1:.2%} ({recall_at_1_count}/{total})")
    print(f"  Recall@5:   {avg_r5:.2%} ({recall_at_5_count}/{total})")
    print("=" * 80)

    output_data = {
        "experiment_config": {
            "document_order": DOCUMENT_ORDER,
            "add_counterfactual": ADD_COUNTERFACTUAL,
            "counterfact_position": COUNTERFACT_POSITION if ADD_COUNTERFACTUAL else None
        },
        "evaluation_summary": {
            "total_questions": total,
            "correct_answers": correct_count,
            "accuracy": accuracy,
            "recall_at_1": avg_r1,
            "recall_at_5": avg_r5,
        },
        "detailed_results": results
    }

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(output_data, f, indent=4, ensure_ascii=False)

    print(f"\nDetailed results saved to: {OUTPUT_FILE}")

